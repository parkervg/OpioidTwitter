{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import text\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/opioid_tweets_label.csv').drop(columns = [\"Unnamed: 0\"])\n",
    "bad_tweets = pd.read_csv('../bad_tweets.txt', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>fav_count</th>\n",
       "      <th>url_present</th>\n",
       "      <th>user_name</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>user_description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.182370e+18</td>\n",
       "      <td>Boston Police, Public Health Officials To Trea...</td>\n",
       "      <td>10/10/19</td>\n",
       "      <td>662.0</td>\n",
       "      <td>True</td>\n",
       "      <td>SaraCarterDC</td>\n",
       "      <td>836793.0</td>\n",
       "      <td>4656.0</td>\n",
       "      <td>@FoxNews Contributor, award winning National S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.182120e+18</td>\n",
       "      <td>Three #Chinese nationals were charged last wee...</td>\n",
       "      <td>10/10/19</td>\n",
       "      <td>302.0</td>\n",
       "      <td>True</td>\n",
       "      <td>EpochTimes</td>\n",
       "      <td>134594.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>An independent, award-winning voice in print &amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.182360e+18</td>\n",
       "      <td>Three #Chinese nationals were charged with imp...</td>\n",
       "      <td>10/10/19</td>\n",
       "      <td>164.0</td>\n",
       "      <td>True</td>\n",
       "      <td>EpochTimes</td>\n",
       "      <td>134594.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>An independent, award-winning voice in print &amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.182720e+18</td>\n",
       "      <td>Boston is using a chemical warfare device to h...</td>\n",
       "      <td>10/11/19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>BUSPH</td>\n",
       "      <td>27642.0</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>The official Twitter of Boston University Scho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.182720e+18</td>\n",
       "      <td>This makes no sense given what President Trump...</td>\n",
       "      <td>10/11/19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>FlagHiApp</td>\n",
       "      <td>1893.0</td>\n",
       "      <td>4547.0</td>\n",
       "      <td>FlagHi™ calculates how temperature, elevation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42933</td>\n",
       "      <td>588727</td>\n",
       "      <td>1.187910e+18</td>\n",
       "      <td>Thanks....haven't got Motrin PM..trying Naprox...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>CarolMc29382003</td>\n",
       "      <td>3729.0</td>\n",
       "      <td>4996.0</td>\n",
       "      <td>#Trump2020#MAGA#KAGA#NRA.No DM NO Dating.. don...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42934</td>\n",
       "      <td>588728</td>\n",
       "      <td>1.187910e+18</td>\n",
       "      <td>@thistallawkgirl One year my husband dressed u...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>beatalley</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>4035.0</td>\n",
       "      <td>Beat Alley-Denver's Music Webzine -  Vintage M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42935</td>\n",
       "      <td>588729</td>\n",
       "      <td>1.187910e+18</td>\n",
       "      <td>I fractured my growth plate when I was 12 and ...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>depressedloc</td>\n",
       "      <td>349.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>Scientist and minor Prophet #FreeSanchez #Free...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42936</td>\n",
       "      <td>588730</td>\n",
       "      <td>1.187910e+18</td>\n",
       "      <td>@_Daks_ Vicodin messed me the fuck up. Like fo...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Road_Block</td>\n",
       "      <td>910.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>Gamer, podcaster, JMM on DungeonDrunks!  RT Si...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42937</td>\n",
       "      <td>588732</td>\n",
       "      <td>1.187890e+18</td>\n",
       "      <td>Does anyone have any morphine, oxy or vicodin?...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Andybeagle1</td>\n",
       "      <td>723.0</td>\n",
       "      <td>2450.0</td>\n",
       "      <td>Been tested by 5 psychiatrists. I'm fine. Sci-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42938 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      tweet_id  \\\n",
       "0           1  1.182370e+18   \n",
       "1           2  1.182120e+18   \n",
       "2           3  1.182360e+18   \n",
       "3           4  1.182720e+18   \n",
       "4           5  1.182720e+18   \n",
       "...       ...           ...   \n",
       "42933  588727  1.187910e+18   \n",
       "42934  588728  1.187910e+18   \n",
       "42935  588729  1.187910e+18   \n",
       "42936  588730  1.187910e+18   \n",
       "42937  588732  1.187890e+18   \n",
       "\n",
       "                                                 content created_at  \\\n",
       "0      Boston Police, Public Health Officials To Trea...   10/10/19   \n",
       "1      Three #Chinese nationals were charged last wee...   10/10/19   \n",
       "2      Three #Chinese nationals were charged with imp...   10/10/19   \n",
       "3      Boston is using a chemical warfare device to h...   10/11/19   \n",
       "4      This makes no sense given what President Trump...   10/11/19   \n",
       "...                                                  ...        ...   \n",
       "42933  Thanks....haven't got Motrin PM..trying Naprox...   10/26/19   \n",
       "42934  @thistallawkgirl One year my husband dressed u...   10/26/19   \n",
       "42935  I fractured my growth plate when I was 12 and ...   10/26/19   \n",
       "42936  @_Daks_ Vicodin messed me the fuck up. Like fo...   10/26/19   \n",
       "42937  Does anyone have any morphine, oxy or vicodin?...   10/26/19   \n",
       "\n",
       "       fav_count url_present        user_name  followers_count  friends_count  \\\n",
       "0          662.0        True     SaraCarterDC         836793.0         4656.0   \n",
       "1          302.0        True       EpochTimes         134594.0          102.0   \n",
       "2          164.0        True       EpochTimes         134594.0          102.0   \n",
       "3            0.0        True            BUSPH          27642.0         2202.0   \n",
       "4            0.0        True        FlagHiApp           1893.0         4547.0   \n",
       "...          ...         ...              ...              ...            ...   \n",
       "42933        2.0        True  CarolMc29382003           3729.0         4996.0   \n",
       "42934        6.0        True        beatalley           1738.0         4035.0   \n",
       "42935        1.0        True     depressedloc            349.0          341.0   \n",
       "42936        0.0       False       Road_Block            910.0          365.0   \n",
       "42937        0.0        True      Andybeagle1            723.0         2450.0   \n",
       "\n",
       "                                        user_description  label  \n",
       "0      @FoxNews Contributor, award winning National S...      0  \n",
       "1      An independent, award-winning voice in print &...      0  \n",
       "2      An independent, award-winning voice in print &...      0  \n",
       "3      The official Twitter of Boston University Scho...      0  \n",
       "4      FlagHi™ calculates how temperature, elevation ...      0  \n",
       "...                                                  ...    ...  \n",
       "42933  #Trump2020#MAGA#KAGA#NRA.No DM NO Dating.. don...      0  \n",
       "42934  Beat Alley-Denver's Music Webzine -  Vintage M...      0  \n",
       "42935  Scientist and minor Prophet #FreeSanchez #Free...      0  \n",
       "42936  Gamer, podcaster, JMM on DungeonDrunks!  RT Si...      0  \n",
       "42937  Been tested by 5 psychiatrists. I'm fine. Sci-...      0  \n",
       "\n",
       "[42938 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of stopwords\n",
    "stops = stopwords.words('english')\n",
    "# Add stop variants without single quotes\n",
    "no_quotes = [re.sub(r'\\'','',word) for word in stops if \"'\" in word]\n",
    "my_stop_words = [\"codeine\", \"hydrocodone\", \"morphine\", \"oxycodone\", \"hydromorphone\", \"fentanyl\", \"oxycontin\", \"vicodin\", \"percocet\"]\n",
    "stops.extend(no_quotes)\n",
    "stops.extend(my_stop_words)\n",
    "def clean_string(string):\n",
    "    # remove HTML entities\n",
    "    temp = re.sub(r'\\&\\w*;','', string)\n",
    "    # remove @user\n",
    "    temp = re.sub(r'@(\\w+)','', temp)\n",
    "    # remove links\n",
    "    temp = re.sub(r'(http|https|ftp)://[a-zA-Z0-9\\\\./]+','', temp)\n",
    "    # lowercase\n",
    "    temp = temp.lower()\n",
    "    # remove hashtags\n",
    "#     temp = re.sub(r'#(\\w+)','', temp)\n",
    "    # remove repeating characters\n",
    "    temp = re.sub(r'(.)\\1{1,}',r'\\1\\1', temp)\n",
    "    # remove non-letters\n",
    "    temp = re.sub(\"[^a-zA-Z]\",\" \", temp)\n",
    "    # remove anything that is less than two characters\n",
    "    temp = re.sub(r'\\b\\w{1,2}\\b','',temp)\n",
    "    # remove multiple spaces\n",
    "    temp = re.sub(r'\\s\\s+', ' ', temp)\n",
    "    return temp\n",
    "\n",
    "def str_preprocess(string):\n",
    "    stemmer = PorterStemmer()\n",
    "    # removing punctuation\n",
    "    removed_punc = ''.join([char for char in string if char not in punctuation])\n",
    "    # removing stopwords\n",
    "    cleaned = [stemmer.stem(word.lower()) for word in removed_punc.split(' ') if word not in stops]\n",
    "    return ' '.join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_stop_words = text.ENGLISH_STOP_WORDS.union([\"codeine\", \"hydrocodone\", \"morphine\", \"oxycodone\", \"hydromorphone\", \"fentanyl\", \"oxycontin\", \"vicodin\", \"percocet\"])\n",
    "docs = df.content.astype(str)\n",
    "cleaned_frame = docs.apply(clean_string).apply(str_preprocess)\n",
    "td_idf_vec = TfidfVectorizer(stop_words=my_stop_words, max_features = 20000)\n",
    "X = td_idf_vec.fit_transform(cleaned_frame)\n",
    "X_norm = normalize(X)\n",
    "X_arr = X_norm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df.drop(columns = [\"id\",\"tweet_id\",\"created_at\", \"user_name\", \"user_description\", \"url_present\", \"content\"])\n",
    "df_final = pd.concat([pd.DataFrame(X_arr),df_x], axis = 1).dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_norm = pd.DataFrame(normalize(df_final.values), columns = df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(df_final_norm, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19994</th>\n",
       "      <th>19995</th>\n",
       "      <th>19996</th>\n",
       "      <th>19997</th>\n",
       "      <th>19998</th>\n",
       "      <th>19999</th>\n",
       "      <th>fav_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269259</td>\n",
       "      <td>0.963068</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.410949</td>\n",
       "      <td>0.911646</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408869</td>\n",
       "      <td>0.912593</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579257</td>\n",
       "      <td>0.815140</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020686</td>\n",
       "      <td>0.715151</td>\n",
       "      <td>0.698664</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>0.509637</td>\n",
       "      <td>0.860355</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.774548</td>\n",
       "      <td>0.632510</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.231469</td>\n",
       "      <td>0.972841</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.442661</td>\n",
       "      <td>0.896689</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714400</td>\n",
       "      <td>0.699720</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34348 rows × 20004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4    5    6    7    8    9  ...  19994  19995  \\\n",
       "952    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "39918  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "35082  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "19271  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "12272  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "6265   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "11284  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "38158  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "860    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "15795  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "\n",
       "       19996  19997  19998  19999  fav_count  followers_count  friends_count  \\\n",
       "952      0.0    0.0    0.0    0.0   0.000000         0.269259       0.963068   \n",
       "39918    0.0    0.0    0.0    0.0   0.004724         0.410949       0.911646   \n",
       "35082    0.0    0.0    0.0    0.0   0.000000         0.408869       0.912593   \n",
       "19271    0.0    0.0    0.0    0.0   0.000000         0.579257       0.815140   \n",
       "12272    0.0    0.0    0.0    0.0   0.020686         0.715151       0.698664   \n",
       "...      ...    ...    ...    ...        ...              ...            ...   \n",
       "6265     0.0    0.0    0.0    0.0   0.005480         0.509637       0.860355   \n",
       "11284    0.0    0.0    0.0    0.0   0.002219         0.774548       0.632510   \n",
       "38158    0.0    0.0    0.0    0.0   0.001342         0.231469       0.972841   \n",
       "860      0.0    0.0    0.0    0.0   0.000437         0.442661       0.896689   \n",
       "15795    0.0    0.0    0.0    0.0   0.000000         0.714400       0.699720   \n",
       "\n",
       "       label  \n",
       "952      0.0  \n",
       "39918    0.0  \n",
       "35082    0.0  \n",
       "19271    0.0  \n",
       "12272    0.0  \n",
       "...      ...  \n",
       "6265     0.0  \n",
       "11284    0.0  \n",
       "38158    0.0  \n",
       "860      0.0  \n",
       "15795    0.0  \n",
       "\n",
       "[34348 rows x 20004 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "import os\n",
    "\n",
    "class OpioidTwitterData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data.values.astype('float'))    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target = self.data[index][-1]\n",
    "        data_val = self.data[index] [:-1]\n",
    "        return data_val,target\n",
    "    \n",
    "train_dataset = OpioidTwitterData(X_train)\n",
    "valid_dataset = OpioidTwitterData(X_test)\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34348 (0%)]\tLoss: 5068.110840\n",
      "Train Epoch: 0 [800/34348 (2%)]\tLoss: 11.151454\n",
      "Train Epoch: 0 [1600/34348 (5%)]\tLoss: 6.541049\n",
      "Train Epoch: 0 [2400/34348 (7%)]\tLoss: 6.883827\n",
      "Train Epoch: 0 [3200/34348 (9%)]\tLoss: 9.421175\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a3b0dc096c41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(20003, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 20003)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "reconstruction_function = nn.MSELoss(size_average=False)\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return BCE + KLD\n",
    "\n",
    "num_epochs = 5\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(img)\n",
    "        loss = loss_function(recon_batch, img, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(img),\n",
    "                len(dataloader.dataset), 100. * batch_idx / len(dataloader),\n",
    "                loss.item() / len(img)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(dataloader.dataset)))\n",
    "    if epoch % 10 == 0:\n",
    "        save = to_img(recon_batch.cpu().data)\n",
    "        save_image(save, './vae_img/image_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './vae.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
