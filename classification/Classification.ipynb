{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import text\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/opioid_tweets_label.csv').drop(columns = [\"Unnamed: 0\"])\n",
    "bad_tweets = pd.read_csv('../bad_tweets.txt', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>fav_count</th>\n",
       "      <th>url_present</th>\n",
       "      <th>user_name</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>user_description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.182370e+18</td>\n",
       "      <td>Boston Police, Public Health Officials To Trea...</td>\n",
       "      <td>10/10/19</td>\n",
       "      <td>662.0</td>\n",
       "      <td>True</td>\n",
       "      <td>SaraCarterDC</td>\n",
       "      <td>836793.0</td>\n",
       "      <td>4656.0</td>\n",
       "      <td>@FoxNews Contributor, award winning National S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.182120e+18</td>\n",
       "      <td>Three #Chinese nationals were charged last wee...</td>\n",
       "      <td>10/10/19</td>\n",
       "      <td>302.0</td>\n",
       "      <td>True</td>\n",
       "      <td>EpochTimes</td>\n",
       "      <td>134594.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>An independent, award-winning voice in print &amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.182360e+18</td>\n",
       "      <td>Three #Chinese nationals were charged with imp...</td>\n",
       "      <td>10/10/19</td>\n",
       "      <td>164.0</td>\n",
       "      <td>True</td>\n",
       "      <td>EpochTimes</td>\n",
       "      <td>134594.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>An independent, award-winning voice in print &amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.182720e+18</td>\n",
       "      <td>Boston is using a chemical warfare device to h...</td>\n",
       "      <td>10/11/19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>BUSPH</td>\n",
       "      <td>27642.0</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>The official Twitter of Boston University Scho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.182720e+18</td>\n",
       "      <td>This makes no sense given what President Trump...</td>\n",
       "      <td>10/11/19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>FlagHiApp</td>\n",
       "      <td>1893.0</td>\n",
       "      <td>4547.0</td>\n",
       "      <td>FlagHi™ calculates how temperature, elevation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42933</td>\n",
       "      <td>588727</td>\n",
       "      <td>1.187910e+18</td>\n",
       "      <td>Thanks....haven't got Motrin PM..trying Naprox...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>CarolMc29382003</td>\n",
       "      <td>3729.0</td>\n",
       "      <td>4996.0</td>\n",
       "      <td>#Trump2020#MAGA#KAGA#NRA.No DM NO Dating.. don...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42934</td>\n",
       "      <td>588728</td>\n",
       "      <td>1.187910e+18</td>\n",
       "      <td>@thistallawkgirl One year my husband dressed u...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>beatalley</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>4035.0</td>\n",
       "      <td>Beat Alley-Denver's Music Webzine -  Vintage M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42935</td>\n",
       "      <td>588729</td>\n",
       "      <td>1.187910e+18</td>\n",
       "      <td>I fractured my growth plate when I was 12 and ...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>depressedloc</td>\n",
       "      <td>349.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>Scientist and minor Prophet #FreeSanchez #Free...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42936</td>\n",
       "      <td>588730</td>\n",
       "      <td>1.187910e+18</td>\n",
       "      <td>@_Daks_ Vicodin messed me the fuck up. Like fo...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Road_Block</td>\n",
       "      <td>910.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>Gamer, podcaster, JMM on DungeonDrunks!  RT Si...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42937</td>\n",
       "      <td>588732</td>\n",
       "      <td>1.187890e+18</td>\n",
       "      <td>Does anyone have any morphine, oxy or vicodin?...</td>\n",
       "      <td>10/26/19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Andybeagle1</td>\n",
       "      <td>723.0</td>\n",
       "      <td>2450.0</td>\n",
       "      <td>Been tested by 5 psychiatrists. I'm fine. Sci-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42938 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      tweet_id  \\\n",
       "0           1  1.182370e+18   \n",
       "1           2  1.182120e+18   \n",
       "2           3  1.182360e+18   \n",
       "3           4  1.182720e+18   \n",
       "4           5  1.182720e+18   \n",
       "...       ...           ...   \n",
       "42933  588727  1.187910e+18   \n",
       "42934  588728  1.187910e+18   \n",
       "42935  588729  1.187910e+18   \n",
       "42936  588730  1.187910e+18   \n",
       "42937  588732  1.187890e+18   \n",
       "\n",
       "                                                 content created_at  \\\n",
       "0      Boston Police, Public Health Officials To Trea...   10/10/19   \n",
       "1      Three #Chinese nationals were charged last wee...   10/10/19   \n",
       "2      Three #Chinese nationals were charged with imp...   10/10/19   \n",
       "3      Boston is using a chemical warfare device to h...   10/11/19   \n",
       "4      This makes no sense given what President Trump...   10/11/19   \n",
       "...                                                  ...        ...   \n",
       "42933  Thanks....haven't got Motrin PM..trying Naprox...   10/26/19   \n",
       "42934  @thistallawkgirl One year my husband dressed u...   10/26/19   \n",
       "42935  I fractured my growth plate when I was 12 and ...   10/26/19   \n",
       "42936  @_Daks_ Vicodin messed me the fuck up. Like fo...   10/26/19   \n",
       "42937  Does anyone have any morphine, oxy or vicodin?...   10/26/19   \n",
       "\n",
       "       fav_count url_present        user_name  followers_count  friends_count  \\\n",
       "0          662.0        True     SaraCarterDC         836793.0         4656.0   \n",
       "1          302.0        True       EpochTimes         134594.0          102.0   \n",
       "2          164.0        True       EpochTimes         134594.0          102.0   \n",
       "3            0.0        True            BUSPH          27642.0         2202.0   \n",
       "4            0.0        True        FlagHiApp           1893.0         4547.0   \n",
       "...          ...         ...              ...              ...            ...   \n",
       "42933        2.0        True  CarolMc29382003           3729.0         4996.0   \n",
       "42934        6.0        True        beatalley           1738.0         4035.0   \n",
       "42935        1.0        True     depressedloc            349.0          341.0   \n",
       "42936        0.0       False       Road_Block            910.0          365.0   \n",
       "42937        0.0        True      Andybeagle1            723.0         2450.0   \n",
       "\n",
       "                                        user_description  label  \n",
       "0      @FoxNews Contributor, award winning National S...      0  \n",
       "1      An independent, award-winning voice in print &...      0  \n",
       "2      An independent, award-winning voice in print &...      0  \n",
       "3      The official Twitter of Boston University Scho...      0  \n",
       "4      FlagHi™ calculates how temperature, elevation ...      0  \n",
       "...                                                  ...    ...  \n",
       "42933  #Trump2020#MAGA#KAGA#NRA.No DM NO Dating.. don...      0  \n",
       "42934  Beat Alley-Denver's Music Webzine -  Vintage M...      0  \n",
       "42935  Scientist and minor Prophet #FreeSanchez #Free...      0  \n",
       "42936  Gamer, podcaster, JMM on DungeonDrunks!  RT Si...      0  \n",
       "42937  Been tested by 5 psychiatrists. I'm fine. Sci-...      0  \n",
       "\n",
       "[42938 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of stopwords\n",
    "stops = stopwords.words('english')\n",
    "# Add stop variants without single quotes\n",
    "no_quotes = [re.sub(r'\\'','',word) for word in stops if \"'\" in word]\n",
    "my_stop_words = [\"codeine\", \"hydrocodone\", \"morphine\", \"oxycodone\", \"hydromorphone\", \"fentanyl\", \"oxycontin\", \"vicodin\", \"percocet\"]\n",
    "stops.extend(no_quotes)\n",
    "stops.extend(my_stop_words)\n",
    "def clean_string(string):\n",
    "    # remove HTML entities\n",
    "    temp = re.sub(r'\\&\\w*;','', string)\n",
    "    # remove @user\n",
    "    temp = re.sub(r'@(\\w+)','', temp)\n",
    "    # remove links\n",
    "    temp = re.sub(r'(http|https|ftp)://[a-zA-Z0-9\\\\./]+','', temp)\n",
    "    # lowercase\n",
    "    temp = temp.lower()\n",
    "    # remove hashtags\n",
    "#     temp = re.sub(r'#(\\w+)','', temp)\n",
    "    # remove repeating characters\n",
    "    temp = re.sub(r'(.)\\1{1,}',r'\\1\\1', temp)\n",
    "    # remove non-letters\n",
    "    temp = re.sub(\"[^a-zA-Z]\",\" \", temp)\n",
    "    # remove anything that is less than two characters\n",
    "    temp = re.sub(r'\\b\\w{1,2}\\b','',temp)\n",
    "    # remove multiple spaces\n",
    "    temp = re.sub(r'\\s\\s+', ' ', temp)\n",
    "    return temp\n",
    "\n",
    "def str_preprocess(string):\n",
    "    stemmer = PorterStemmer()\n",
    "    # removing punctuation\n",
    "    removed_punc = ''.join([char for char in string if char not in punctuation])\n",
    "    # removing stopwords\n",
    "    cleaned = [stemmer.stem(word.lower()) for word in removed_punc.split(' ') if word not in stops]\n",
    "    return ' '.join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_stop_words = text.ENGLISH_STOP_WORDS.union([\"codeine\", \"hydrocodone\", \"morphine\", \"oxycodone\", \"hydromorphone\", \"fentanyl\", \"oxycontin\", \"vicodin\", \"percocet\"])\n",
    "docs = df.content.astype(str)\n",
    "cleaned_frame = docs.apply(clean_string).apply(str_preprocess)\n",
    "td_idf_vec = TfidfVectorizer(stop_words=my_stop_words, max_features = 20000)\n",
    "X = td_idf_vec.fit_transform(cleaned_frame)\n",
    "X_norm = normalize(X)\n",
    "X_arr = X_norm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df.drop(columns = [\"id\",\"tweet_id\",\"created_at\", \"user_name\", \"user_description\", \"url_present\", \"content\"])\n",
    "df_final = pd.concat([df_x,pd.DataFrame(X_arr)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final.drop('label', axis = 1), df_final['label'], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7af2450db761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "batch_size = 8\n",
    "train_target = torch.tensor(y_train.values.astype(np.float32))\n",
    "train = torch.tensor(X_train.values.astype(np.float32)) \n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1fc362bcb080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_x_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda36d8fccebcaa41dba6e21681695b9207"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
