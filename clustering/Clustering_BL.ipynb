{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem & Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Cleaning/opioid_tweets_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation, urls)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = [] # List of the stemmed words\n",
    "totalvocab_tokenized = [] #List of the tokenized words\n",
    "for i in df[\"content\"].values.astype(\"U\"):\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664382 664382\n"
     ]
    }
   ],
   "source": [
    "totalvocab_stemmed[1]\n",
    "print( len(totalvocab_stemmed), len(totalvocab_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    # remove @user\n",
    "    temp = re.sub(r'@(\\w+)','', string) # clearly we do not wish to look at the user name.\n",
    "        # remove links\n",
    "    temp = re.sub(r'https://[a-zA-Z0-9\\\\./]+\\s*','', temp) # had to modify this portion from Brenans, as his didn't clear http \n",
    "     # remove non-letters\n",
    "    temp = re.sub(\"[^a-zA-Z]\",\" \", temp) # Will get rid of hashtags, $, etc.\n",
    "        # remove multiple spaces\n",
    "    temp = re.sub(r'\\s\\s+', ' ', temp) \n",
    "        # remove anything that is less than two characters\n",
    "    temp = re.sub(r'\\b\\w{1,2}\\b','',temp) # s will be an 'important character' if we do not include this\n",
    "    # I am assuming that is because s is not a stopword in nltk. we could adjust by getting ride of any single character\n",
    "    return temp\n",
    "\n",
    "content_clean = []\n",
    "for i in df[\"content\"].values.astype(\"U\"):\n",
    "    content_clean.append(clean_string(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a low min_df 'codein crazi' is a reference to the artist \"Future\", and might produce too much noise. Consider raising the df or adding it to the stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JungleBook\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.3 s\n",
      "(42936, 20091)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=.9, max_features=200000,\n",
    "                                 min_df = 1, stop_words= 'english' ,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                   strip_accents = \"unicode\", lowercase = True )\n",
    " \n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(content_clean)\n",
    "#The warning message is present, despite adding strip_accent, in addition I believe I should add a preprocess\n",
    "#stop words from nltk. Setting stop_words = 'english' uses sklearn's stopwords\n",
    "\n",
    "print(tfidf_matrix.shape) #using min_df = .1, returns size of (42936, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JungleBook\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_matrix.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(tfidf_matrix, 'tfidf_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accent', 'accentu', 'accept', 'access', 'accessori', 'accesswir', 'acci', 'accid', 'accident', 'acclimatis']\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(terms[100:110]) # Whether we use sklean, or nltk stopwords, we will end up with 'like'. I am very suprised this is not \n",
    "# a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabdic = tfidf_vectorizer.vocabulary_ #Is dictonary of our terms\n",
    "#print(vocabdic) # finna be usefull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some terms are too vague, perhaps we could add those to the  stop words and find the next m tweets that will lead to illicit drugs. The bad tweets document showed that (purchase, buy) where common in tweets selling drugs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42936, 20091)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance between tweets are calculated\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, each tweet is compared to each other, forming a matrix with(num_tweets, num_tweets) shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2fcc3956a5ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnum_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dist' is not defined"
     ]
    }
   ],
   "source": [
    "print(dist.shape) \n",
    "num_tweets = dist.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing one tweet to itself will, as expected, return a distance of 0 (or practically small enough to be zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist[5][5] # Distance between tweet 5 and tweet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist[5][54] # Distance between tweet 5 and tweet 54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using a Nearest-Neighbors-like approach to sniff out bad tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have a known illegal tweet serving as an archetypical \"bad tweet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tweets, like 584417, @ high profile celebrities\n",
    "# Probably so when a fan is looking at the celeb mentions, they see the tweet\n",
    "# Maybe a counter-argument to removing all @ signs\n",
    "\"\"\"\n",
    "Tough calls:\n",
    "    383147\n",
    "    Does an account called adspostinghere that happens to host an ad selling codeine count?\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "target_id = '584417'\n",
    "df[df[\"id\"] == target_id][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = df.index[df[\"id\"] == target_id].tolist()[0]\n",
    "content_clean[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = []\n",
    "for i in range(num_tweets):\n",
    "    sorted_scores.append((i, dist[ix][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = sorted(sorted_scores, key = lambda x: x[1])\n",
    "sorted_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = [x[0] for x in sorted_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up existing bad tweets\n",
    "with open(\"../bad_tweets.txt\", \"r\") as f:\n",
    "    bad_ids = [i.strip() for i in f.read().split(\"\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted_scores[:100]:\n",
    "    if df.iloc[x][\"id\"] not in bad_ids:\n",
    "        print(content_clean[x])\n",
    "        print(\"TWEET ID:\")\n",
    "        print(df.iloc[x][\"id\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K - Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TESTING FOR THE OPTIMAL NUMBER OF K CLUSTERS FOR THE KMEANS MODEL\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss =[] # Within cluster sum of squares\n",
    "for i in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(range(1, 10), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show() # showing plot from a prior tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the optimal number would is the number of terms we have from tfidf. Brenden worked his plot using .score(). I would consider doing it his was to find the correct amount of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIC method for determining K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BIC metods works by treating each blob of data as multiple datasets with seperate means and variances. From there determining the mean and vairance. Giving us a BIC score that will tell us the amount of cluster is best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sp.sparse.csr_matrix.toarray(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.43 GiB for an array with shape (42936, 20091) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-25f59c37ba9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgm_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mgm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianMixture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BIC for number of cluster(s) {}: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Log-likelihood score for number of cluster(s) {}: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \"\"\"\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdo_init\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mlower_bound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfty\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdo_init\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower_bound_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\mixture\\_base.py\u001b[0m in \u001b[0;36m_initialize_parameters\u001b[1;34m(self, X, random_state)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n\u001b[1;32m--> 147\u001b[1;33m                                    random_state=random_state).fit(X).labels_\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[0mresp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_params\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'random'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[1;32m--> 859\u001b[1;33m                         order=order, copy=self.copy_x)\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmay_share_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     if (warn_on_dtype and dtypes_orig is not None and\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.43 GiB for an array with shape (42936, 20091) and data type float64"
     ]
    }
   ],
   "source": [
    "gm_bic= []\n",
    "gm_score=[]\n",
    "for i in range(1,4):\n",
    "    gm = GaussianMixture(n_components=i,n_init=10,tol=1e-3,max_iter=1000).fit(X)\n",
    "    print(\"BIC for number of cluster(s) {}: {}\".format(i,gm.bic(X)))\n",
    "    print(\"Log-likelihood score for number of cluster(s) {}: {}\".format(i,gm.score(X)))\n",
    "    print(\"-\"*100)\n",
    "    gm_bic.append(-gm.bic(X))\n",
    "    gm_score.append(gm.score(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So my method only works for smaller data sets or great computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K -Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 9 #using num clusters from a prior term list.\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist() # The cluster is simply an array of non-negative integers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl') # saves results for km as a pickle document\n",
    "km = joblib.load('doc_cluster.pkl') \n",
    "clusters = km.labels_.tolist() # Tells you what word, belongs to what group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'clusters' : clusters, 'words': content_clean }\n",
    "frame = pd.DataFrame( data , index = [clusters] , columns = [ 'words' , 'clusters'] )\n",
    "#created a frame than can classify the rows clean rows of df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['clusters'].value_counts() #This tell you what each string, in df['content'] belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    for ind in order_centroids[i, :9]:\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
