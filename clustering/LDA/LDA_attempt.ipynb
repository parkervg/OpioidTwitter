{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Let's do this thing. \n",
    "\n",
    "# Basic Info\n",
    "We want to create an LDA model for the Opioid Tweets.\n",
    "The primary resource upon creating this notebook is [the following page](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html). Most the meat is from that site but I'm sure there'll be plenty things we'll need to add for our own purpose. This is just to get the ball rolling. \n",
    "\n",
    "The current skeleton for what needs to be done, along with associated questions:\n",
    "* First, we need to establish some basic ideas for the tweets we will be analyzing:\n",
    "    * We need to make sure that the only part of the data we are using is the actual text of the tweet. \n",
    "    * Our tokenization should be wary of contractions; i.e. we need to decide whether it's important that \"don't\" does not seperate into \"don\" and \"t\". \n",
    "    * We need to decide how the model will interpret emojis; i.e. will/can it interpret them to be their own words or will it just be more apt to remove them altogther?\n",
    "    * Will we remove links completely or will we replace each link with a common, unique word (like \"tco\" to represent a `t.co` link) to give the model a chance to group  tweets with links together?\n",
    "    * A similar question arises with regard to hashtags.\n",
    "* Next, we create the model. \n",
    "    * Should we consider trying/comparing different token/stop-words/stem packages?\n",
    "    * How many iterations? topics? \n",
    "* And finally evaluate, improve, and compare to the other model types. \n",
    "    * What areas can be improved and how? (hyperparameters)\n",
    "    * How do we evaluate the success of the model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "We import the necessary libraries and methods, including those of the Twitter data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "df = pd.read_csv(\"data/opioid_tweets.csv\")\n",
    "\n",
    "for i in df['content']:\n",
    "    tweets.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I test and see there are a few non-string values in `tweets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42954\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "error. index: 14828\n",
      "<class 'float'>\n",
      "nan\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "print(len(tweets))\n",
    "for i in tweets:\n",
    "    if type(i) != str:\n",
    "        print('error. index: ' + str(tweets.index(i)))\n",
    "        counter+=1\n",
    "        print(type(i))\n",
    "        print(i)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as though there were 14 `nan` values of type `float` in the `tweets` list up until this point. All of them showed index 14828. This line removes them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [i for i in tweets if type(i) != float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the tokenizing, establishing of stop-words, and stemming needed for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I ensure common elements in links do not get included in the corpus (e.g. 'http' or the 't' and 'co' in \"t.co\", etc. ) The reason for the `for` loop is so I don't have to resart the kernel everytime I test something new out. Otherwise this cell is simply `en_stop += ['co', 't', 'http']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_stop_words = ['co', 't', 'http', 's']\n",
    "for i in added_stop_words:\n",
    "    if i not in en_stop:\n",
    "        en_stop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in tweets:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the model where `num_topics=2` means that we are primarily looking for 2 general topics to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.053*\"http\" + 0.027*\"morphin\" + 0.021*\"codein\" + 0.015*\"crazi\" + 0.013*\"pain\"'), (1, '0.076*\"http\" + 0.050*\"fentanyl\" + 0.014*\"oxycontin\" + 0.014*\"drug\" + 0.010*\"heroin\"'), (2, '0.030*\"http\" + 0.025*\"percocet\" + 0.019*\"codein\" + 0.018*\"like\" + 0.015*\"vicodin\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=5, num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident, we need to account for links like those that start with `http` or the twitter version `t.co`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to work on\n",
    "* As of now, the main priority is accounting for the links. \n",
    "* Another thing to address after the links is the prominent theme of Future's \"Codeine Crazy\" and Apple Music coming up. \n",
    "* Once the above issues are taken care of, the main focus shifts to tweaking the model to give the most helpful image of the data and how to compare it to the other models the team has tried. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
